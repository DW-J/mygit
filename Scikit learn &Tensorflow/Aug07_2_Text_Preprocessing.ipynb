{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPyo5g92fm7VobLQKDklOFh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 패딩 (Padding)\n","\n","자연어 처리 진행 중 각각의 문장마다 길이가 서로 다름\n","\n","컴퓨터 입장 : 길이가 동일한 문장의 경우에는 하나의 행렬로 묶어서 보게 되고, 그걸로 한번에 처리가 가능\n","\n","다양한 문장 길이를 임의적으로 동일하게 맞춰주는 작업을 패딩 (Padding)"],"metadata":{"id":"SDyNC5PYkp-T"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"KjQbzE46kZze","executionInfo":{"status":"ok","timestamp":1722996770644,"user_tz":-540,"elapsed":347,"user":{"displayName":"비오","userId":"10385839396547807732"}}},"outputs":[],"source":["import numpy as np\n","from tensorflow.keras.preprocessing.text import Tokenizer"]},{"cell_type":"code","source":["final_sentence = [['twinkle', 'twinkle', 'little', 'star'], ['wonder'], ['world', 'high'], ['like', 'diamond', 'sky'], ['twinkle', 'twinkle', 'little', 'star'], ['wonder'], ['twinkle', 'twinkle', 'little', 'star'], ['wonder'], ['world', 'high'], ['like', 'diamond', 'sky'], ['twinkle', 'twinkle', 'little', 'star'], ['wonder'], ['london', 'bridge', 'falling'], ['falling', 'falling'], ['london', 'bridge', 'falling'], ['fair', 'lady'], ['build', 'iron', 'bars'], ['iron', 'bars', 'iron', 'bars'], ['build', 'iron', 'bars'], ['fair', 'lady'], ['iron', 'bars', 'bend', 'break'], ['bend', 'break', 'bend', 'break'], ['iron', 'bars', 'bend', 'break'], ['fair', 'lady'], ['build', 'gold', 'silver'], ['gold', 'silver', 'gold', 'silver'], ['build', 'gold', 'silver'], ['fair', 'lady'], ['gold', 'silver', \"'ve\", 'got'], [\"'ve\", 'got', \"'ve\", 'got'], ['gold', 'silver', \"'ve\", 'got'], ['fair', 'lady'], ['london', 'bridge', 'falling'], ['falling', 'falling'], ['london', 'bridge', 'falling'], ['fair', 'lady'], ['london', 'bridge', 'falling'], ['falling', 'falling'], ['london', 'bridge', 'falling'], ['fair', 'lady']]\n","print(final_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j5UVsNEMlVTe","executionInfo":{"status":"ok","timestamp":1722996772057,"user_tz":-540,"elapsed":14,"user":{"displayName":"비오","userId":"10385839396547807732"}},"outputId":"e30a52db-b46c-49ea-bec4-65c79976439e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["[['twinkle', 'twinkle', 'little', 'star'], ['wonder'], ['world', 'high'], ['like', 'diamond', 'sky'], ['twinkle', 'twinkle', 'little', 'star'], ['wonder'], ['twinkle', 'twinkle', 'little', 'star'], ['wonder'], ['world', 'high'], ['like', 'diamond', 'sky'], ['twinkle', 'twinkle', 'little', 'star'], ['wonder'], ['london', 'bridge', 'falling'], ['falling', 'falling'], ['london', 'bridge', 'falling'], ['fair', 'lady'], ['build', 'iron', 'bars'], ['iron', 'bars', 'iron', 'bars'], ['build', 'iron', 'bars'], ['fair', 'lady'], ['iron', 'bars', 'bend', 'break'], ['bend', 'break', 'bend', 'break'], ['iron', 'bars', 'bend', 'break'], ['fair', 'lady'], ['build', 'gold', 'silver'], ['gold', 'silver', 'gold', 'silver'], ['build', 'gold', 'silver'], ['fair', 'lady'], ['gold', 'silver', \"'ve\", 'got'], [\"'ve\", 'got', \"'ve\", 'got'], ['gold', 'silver', \"'ve\", 'got'], ['fair', 'lady'], ['london', 'bridge', 'falling'], ['falling', 'falling'], ['london', 'bridge', 'falling'], ['fair', 'lady'], ['london', 'bridge', 'falling'], ['falling', 'falling'], ['london', 'bridge', 'falling'], ['fair', 'lady']]\n"]}]},{"cell_type":"code","source":["# 정수 인코딩\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(final_sentence)\n","aa = tokenizer.texts_to_sequences(final_sentence)\n","print(aa)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C2Xaw340lcuQ","executionInfo":{"status":"ok","timestamp":1722996773444,"user_tz":-540,"elapsed":7,"user":{"displayName":"비오","userId":"10385839396547807732"}},"outputId":"76b61fe7-0b2b-4d47-d2b8-9effc26ee1af"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[[2, 2, 11, 12], [13], [19, 20], [21, 22, 23], [2, 2, 11, 12], [13], [2, 2, 11, 12], [13], [19, 20], [21, 22, 23], [2, 2, 11, 12], [13], [5, 6, 1], [1, 1], [5, 6, 1], [3, 4], [14, 7, 8], [7, 8, 7, 8], [14, 7, 8], [3, 4], [7, 8, 15, 16], [15, 16, 15, 16], [7, 8, 15, 16], [3, 4], [14, 9, 10], [9, 10, 9, 10], [14, 9, 10], [3, 4], [9, 10, 17, 18], [17, 18, 17, 18], [9, 10, 17, 18], [3, 4], [5, 6, 1], [1, 1], [5, 6, 1], [3, 4], [5, 6, 1], [1, 1], [5, 6, 1], [3, 4]]\n"]}]},{"cell_type":"code","source":["# 가장 길이가 긴 문장의 길이가 얼마나 되는지\n","\n","long_sentence = max(len(item) for item in aa)\n","\n","print(long_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HeTpPmUDlx1b","executionInfo":{"status":"ok","timestamp":1722996774559,"user_tz":-540,"elapsed":8,"user":{"displayName":"비오","userId":"10385839396547807732"}},"outputId":"7f6cc73d-4b7f-4d4a-9308-759b029f4360"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["4\n"]}]},{"cell_type":"code","source":["# 모든 문장의 길이를 가장 길이가 긴 문장의 크기만큼 맞춰주기\n","# 임의의(가상의) 단어가 있다고 가정한 후 => 이 단어의 인덱스는 0\n","# ex) 최대 길이가 4일때, 문장의 길이가 4보다 짧으면 4가 될때까지 0으로 채우는...\n","\n","for item in aa:\n","  while len(item) < long_sentence:\n","    item.append(0)\n","\n","n = np.array(aa)\n","n\n","\n","# 패딩 : Data에 특정한 값을 넣어서 data의 shape(크기)를 조정하는 것!"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4lxiFeA2mIMJ","executionInfo":{"status":"ok","timestamp":1722996776701,"user_tz":-540,"elapsed":359,"user":{"displayName":"비오","userId":"10385839396547807732"}},"outputId":"d55a3762-ddf3-4347-d50b-ea7fc2555647"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 2,  2, 11, 12],\n","       [13,  0,  0,  0],\n","       [19, 20,  0,  0],\n","       [21, 22, 23,  0],\n","       [ 2,  2, 11, 12],\n","       [13,  0,  0,  0],\n","       [ 2,  2, 11, 12],\n","       [13,  0,  0,  0],\n","       [19, 20,  0,  0],\n","       [21, 22, 23,  0],\n","       [ 2,  2, 11, 12],\n","       [13,  0,  0,  0],\n","       [ 5,  6,  1,  0],\n","       [ 1,  1,  0,  0],\n","       [ 5,  6,  1,  0],\n","       [ 3,  4,  0,  0],\n","       [14,  7,  8,  0],\n","       [ 7,  8,  7,  8],\n","       [14,  7,  8,  0],\n","       [ 3,  4,  0,  0],\n","       [ 7,  8, 15, 16],\n","       [15, 16, 15, 16],\n","       [ 7,  8, 15, 16],\n","       [ 3,  4,  0,  0],\n","       [14,  9, 10,  0],\n","       [ 9, 10,  9, 10],\n","       [14,  9, 10,  0],\n","       [ 3,  4,  0,  0],\n","       [ 9, 10, 17, 18],\n","       [17, 18, 17, 18],\n","       [ 9, 10, 17, 18],\n","       [ 3,  4,  0,  0],\n","       [ 5,  6,  1,  0],\n","       [ 1,  1,  0,  0],\n","       [ 5,  6,  1,  0],\n","       [ 3,  4,  0,  0],\n","       [ 5,  6,  1,  0],\n","       [ 1,  1,  0,  0],\n","       [ 5,  6,  1,  0],\n","       [ 3,  4,  0,  0]])"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","source":["# Keras의 pad_sequences"],"metadata":{"id":"fAY0XECZnLI-"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.sequence import pad_sequences"],"metadata":{"id":"auF_XVZZnRzq","executionInfo":{"status":"ok","timestamp":1722996848906,"user_tz":-540,"elapsed":10,"user":{"displayName":"비오","userId":"10385839396547807732"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# 위에 있는 aa를 원상태로 복구\n","aa = tokenizer.texts_to_sequences(final_sentence)\n","print(aa)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"plYMH96enYHV","executionInfo":{"status":"ok","timestamp":1722996913425,"user_tz":-540,"elapsed":364,"user":{"displayName":"비오","userId":"10385839396547807732"}},"outputId":"36f6fdde-ebea-4ad4-d093-4c163a8a6a37"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[[2, 2, 11, 12], [13], [19, 20], [21, 22, 23], [2, 2, 11, 12], [13], [2, 2, 11, 12], [13], [19, 20], [21, 22, 23], [2, 2, 11, 12], [13], [5, 6, 1], [1, 1], [5, 6, 1], [3, 4], [14, 7, 8], [7, 8, 7, 8], [14, 7, 8], [3, 4], [7, 8, 15, 16], [15, 16, 15, 16], [7, 8, 15, 16], [3, 4], [14, 9, 10], [9, 10, 9, 10], [14, 9, 10], [3, 4], [9, 10, 17, 18], [17, 18, 17, 18], [9, 10, 17, 18], [3, 4], [5, 6, 1], [1, 1], [5, 6, 1], [3, 4], [5, 6, 1], [1, 1], [5, 6, 1], [3, 4]]\n"]}]},{"cell_type":"code","source":["# kerasPadding = pad_sequences(aa)    # 데이터의 앞에서부터 0을 채움\n","kerasPadding = pad_sequences(aa, padding='post')    # 데이터의 뒤에 0을 채움\n","kerasPadding"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"111T19O6noD5","executionInfo":{"status":"ok","timestamp":1722997086757,"user_tz":-540,"elapsed":422,"user":{"displayName":"비오","userId":"10385839396547807732"}},"outputId":"4791ea4f-8bdb-46e9-f3b3-cd7e9e714547"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 2,  2, 11, 12],\n","       [13,  0,  0,  0],\n","       [19, 20,  0,  0],\n","       [21, 22, 23,  0],\n","       [ 2,  2, 11, 12],\n","       [13,  0,  0,  0],\n","       [ 2,  2, 11, 12],\n","       [13,  0,  0,  0],\n","       [19, 20,  0,  0],\n","       [21, 22, 23,  0],\n","       [ 2,  2, 11, 12],\n","       [13,  0,  0,  0],\n","       [ 5,  6,  1,  0],\n","       [ 1,  1,  0,  0],\n","       [ 5,  6,  1,  0],\n","       [ 3,  4,  0,  0],\n","       [14,  7,  8,  0],\n","       [ 7,  8,  7,  8],\n","       [14,  7,  8,  0],\n","       [ 3,  4,  0,  0],\n","       [ 7,  8, 15, 16],\n","       [15, 16, 15, 16],\n","       [ 7,  8, 15, 16],\n","       [ 3,  4,  0,  0],\n","       [14,  9, 10,  0],\n","       [ 9, 10,  9, 10],\n","       [14,  9, 10,  0],\n","       [ 3,  4,  0,  0],\n","       [ 9, 10, 17, 18],\n","       [17, 18, 17, 18],\n","       [ 9, 10, 17, 18],\n","       [ 3,  4,  0,  0],\n","       [ 5,  6,  1,  0],\n","       [ 1,  1,  0,  0],\n","       [ 5,  6,  1,  0],\n","       [ 3,  4,  0,  0],\n","       [ 5,  6,  1,  0],\n","       [ 1,  1,  0,  0],\n","       [ 5,  6,  1,  0],\n","       [ 3,  4,  0,  0]], dtype=int32)"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["# 기준을 최대길이로 할 필요가 없는 경우에 'maxlen'을 이용해서 원하는 길이 조절 가능\n","# 원하는 길이보다 긴 문장이 있는 경우, 데이터 해당하는 갯수만큼 손실\n","# 기본적으로 문장의 맨 앞에서부터 차례대로 데이터의 손실이 발생하고,\n","# 데이터 손실을 맨 뒤에서부터 받고 싶다면, truncating='post' 옵션을 추가해서...!\n","kerasPadding2 = pad_sequences(aa, padding='post', truncating='post', maxlen=3)\n","kerasPadding2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"87FrPg5moU9M","executionInfo":{"status":"ok","timestamp":1722997381503,"user_tz":-540,"elapsed":350,"user":{"displayName":"비오","userId":"10385839396547807732"}},"outputId":"d50e6f91-4b86-4706-f38d-33ef8f441793"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 2,  2, 11],\n","       [13,  0,  0],\n","       [19, 20,  0],\n","       [21, 22, 23],\n","       [ 2,  2, 11],\n","       [13,  0,  0],\n","       [ 2,  2, 11],\n","       [13,  0,  0],\n","       [19, 20,  0],\n","       [21, 22, 23],\n","       [ 2,  2, 11],\n","       [13,  0,  0],\n","       [ 5,  6,  1],\n","       [ 1,  1,  0],\n","       [ 5,  6,  1],\n","       [ 3,  4,  0],\n","       [14,  7,  8],\n","       [ 7,  8,  7],\n","       [14,  7,  8],\n","       [ 3,  4,  0],\n","       [ 7,  8, 15],\n","       [15, 16, 15],\n","       [ 7,  8, 15],\n","       [ 3,  4,  0],\n","       [14,  9, 10],\n","       [ 9, 10,  9],\n","       [14,  9, 10],\n","       [ 3,  4,  0],\n","       [ 9, 10, 17],\n","       [17, 18, 17],\n","       [ 9, 10, 17],\n","       [ 3,  4,  0],\n","       [ 5,  6,  1],\n","       [ 1,  1,  0],\n","       [ 5,  6,  1],\n","       [ 3,  4,  0],\n","       [ 5,  6,  1],\n","       [ 1,  1,  0],\n","       [ 5,  6,  1],\n","       [ 3,  4,  0]], dtype=int32)"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["# One-Hot Encoding (원-핫 인코딩)\n","\n","컴퓨터 : 문자보다 숫자를 더 잘 처리함\n","\n","문자를 숫자로 바꾸는 방법 중 하나\n","\n","데이터를 수많은 0과 한개의 1값으로 데이터를 구별하는 인코딩\n","\n","단어 집합의 크기를 vector의 차원(1차원)으로 지정하고, 표현하고자 하는 단어의 인덱스에 1로 부여 / 다른 단어에는 0을 부여하는 단어의 벡터\n","\n","- 벡터(Vector) : 공간에서 크기와 방향을 가진 것, 순서가 존재하는 리스트 !\n","ex) [190.3, 81.4] != [81.4, 190.3]"],"metadata":{"id":"mAYJlC2Vpo9g"}},{"cell_type":"code","source":["!pip install konlpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"MMXnPkXBqkJf","executionInfo":{"status":"ok","timestamp":1722997704422,"user_tz":-540,"elapsed":6576,"user":{"displayName":"비오","userId":"10385839396547807732"}},"outputId":"afdda76e-a87b-494e-b8a4-5afe9c8c57ad"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting konlpy\n","  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n","Collecting JPype1>=0.7.0 (from konlpy)\n","  Downloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n","Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from konlpy) (4.9.4)\n","Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.10/dist-packages (from konlpy) (1.26.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from JPype1>=0.7.0->konlpy) (24.1)\n","Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading JPype1-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (488 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.6/488.6 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: JPype1, konlpy\n","Successfully installed JPype1-1.5.0 konlpy-0.6.0\n"]}]},{"cell_type":"code","source":["from konlpy.tag import Okt"],"metadata":{"id":"hf0_6_YBq-LU","executionInfo":{"status":"ok","timestamp":1722997806275,"user_tz":-540,"elapsed":7,"user":{"displayName":"비오","userId":"10385839396547807732"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["okt = Okt()\n","word_token = okt.morphs(\"오늘은 어째서 금요일이 아니죠?\")\n","print(word_token)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FNUTOWSzqv8K","executionInfo":{"status":"ok","timestamp":1722997819637,"user_tz":-540,"elapsed":11988,"user":{"displayName":"비오","userId":"10385839396547807732"}},"outputId":"8f41d526-6d95-4498-c1b2-409708c12631"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["['오늘', '은', '어째서', '금요일', '이', '아니죠', '?']\n"]}]},{"cell_type":"code","source":["# 나눠진 단어에 각각 index 부여하기 (0부터 시작)\n","# enumerate를 통해서 자동으로 0부터 인덱스가 부여되도록\n","word_index = {w : i for i, w in enumerate(word_token)}\n","print(word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JLq3PkV8q628","executionInfo":{"status":"ok","timestamp":1722997875022,"user_tz":-540,"elapsed":339,"user":{"displayName":"비오","userId":"10385839396547807732"}},"outputId":"697d8ea6-7612-4be7-cb9f-fad1d130153b"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["{'오늘': 0, '은': 1, '어째서': 2, '금요일': 3, '이': 4, '아니죠': 5, '?': 6}\n"]}]},{"cell_type":"code","source":["# 토큰을 입력하면, 토큰에 대한 원-핫 벡터를 뽑아내는 함수\n","def ohe(w, word_index):\n","  ohVector = [0] * (len(word_index))    # 위에서 만든 index의 길이만큼 0을 채운 list를 생성\n","  index = word_index[w]   # 파라미터로 넣은 단어를 넣은 변수\n","  ohVector[index] = 1     # 해당변수가 있는 위치를 1값으로 변경\n","  return ohVector"],"metadata":{"id":"utr3yft1rSop","executionInfo":{"status":"ok","timestamp":1722998092248,"user_tz":-540,"elapsed":339,"user":{"displayName":"비오","userId":"10385839396547807732"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# '금요일' 이라는 단어의 원-핫 벡터\n","ohe('금요일', word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"112gobMasHhq","executionInfo":{"status":"ok","timestamp":1722998123124,"user_tz":-540,"elapsed":316,"user":{"displayName":"비오","userId":"10385839396547807732"}},"outputId":"50595550-1c65-45d5-8c32-64bb01ed1acb"},"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 0, 0, 1, 0, 0, 0]"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","source":["# Keras를 이용한 원-핫 인코딩"],"metadata":{"id":"LsvZVTIKyE76"}},{"cell_type":"code","source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.utils import to_categorical"],"metadata":{"id":"OWcB-_2LyI-2","executionInfo":{"status":"ok","timestamp":1722999724524,"user_tz":-540,"elapsed":12,"user":{"displayName":"비오","userId":"10385839396547807732"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["text = \"곰 세 마리가 한 집에 있어 아빠 곰 엄마 곰 애기 곰\"\n","\n","# 빈도수 기준으로 단어 집합 만들기\n","tokenizer = Tokenizer()\n","# fit_on_texts 함수가 리스트 형태의 입력을 요구함\n","tokenizer.fit_on_texts([text])\n","print(tokenizer.word_index)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LQw54mrDye0n","executionInfo":{"status":"ok","timestamp":1722999827666,"user_tz":-540,"elapsed":346,"user":{"displayName":"비오","userId":"10385839396547807732"}},"outputId":"3c87b5f1-cff0-47c8-8a0a-3abcc4cbb9a2"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["{'곰': 1, '세': 2, '마리가': 3, '한': 4, '집에': 5, '있어': 6, '아빠': 7, '엄마': 8, '애기': 9}\n"]}]},{"cell_type":"code","source":["# 생성된 단어 집합 내의 일부 단어들로만 구성된 서브 텍스트를 하나 생성\n","text2 = \"아빠 곰 세 마리가\"\n","\n","# texts_to_sequences는 리스트 [\"아빠 곰 세 마리가\"]를 입력 (리스트 형태의 입력을 요구함)\n","# 이 함수는 입력 리스트의 각 요소(텍스트)를 숫자로 변환해서 리스트 형태로 반환\n","# 이 함수의 반환 값이 [[7, 1, 2, 3]]이므로, 이중 리스트 형태\n","# 즉, 리스트 안의 텍스트를 변환하여 리스트로 return하기 때문에,\n","#   단일 텍스트 변환 결과를 얻기 위해서 0번째 요소를 가져왔음\n","\n","bear = tokenizer.texts_to_sequences([text2])[0]\n","print(bear)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XncvN9gJy4-q","executionInfo":{"status":"ok","timestamp":1722999978124,"user_tz":-540,"elapsed":15,"user":{"displayName":"비오","userId":"10385839396547807732"}},"outputId":"416b9d95-9e01-4c43-cf5b-a09cda88c89c"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["[7, 1, 2, 3]\n"]}]},{"cell_type":"code","source":["# to_categorical() : keras에서 원-핫 인코딩하는 함수\n","cate = to_categorical(bear)\n","print(cate)\n","\n","# [7, 1, 2, 3] 을 표현한 결과\n","# 1. 단어 집합의 인덱스 시작 숫자는 1\n","# 2. 컴퓨터는 0부터 시작\n","# 3. 각 list의 0번째 자리는 임의적으로 0으로 채워두고\n","# 4. 각각의 인덱스에 해당하는 자리의 값이 1로 바뀌도록!!"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gzwis7LQ0FOv","executionInfo":{"status":"ok","timestamp":1723000337922,"user_tz":-540,"elapsed":367,"user":{"displayName":"비오","userId":"10385839396547807732"}},"outputId":"e341a095-6b60-4be0-fc6f-80246cee8ddb"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0. 0. 0. 0. 0. 0. 0. 1.]\n"," [0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 1. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0.]]\n"]}]},{"cell_type":"markdown","source":["# 장점 :\n","\n","  문자데이터 경우에는 컴퓨터가 처리하기 쉽게 숫자로 변환\n","\n","# 단점 :\n","\n","  1. 단어의 갯수가 늘어날수록, 벡터를 저장하기 위한 공간이 늘어남 (= 벡터의 차원이 늘어남)\n","  2. 원-핫 벡터는 단어의 유사도를 표현하지 못한다는 단점\n","\n","  (추천 시스템의 경우)\n","\n","  특정 검색어에 대해서 유사 단어에 대한 결과도 함께 보여줄 수 있어야 하는데, 단어간의 유사성을 계산할 수 없다면, 연관 검색어를 보여줄 수 없을 것!\n","\n","  이러한 단점을 해결하기 위해서 단어의 잠재 의미를 반영해서 다차원 공간에 벡터화 하는 기법을 사용할건데 => 대표적으로 예측 기반으로 벡터화하는 Word2Vec이 있음"],"metadata":{"id":"BqeL6r6C0BsH"}}]}